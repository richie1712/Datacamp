# Import the necessary modules

from nltk.tokenize import regexp_tokenize

from nltk.tokenize import TweetTokenizer



# Define a regex pattern to find hashtags: pattern1

pattern1 = r"#\w+"



# Use the pattern on the first tweet in the tweets list

regexp_tokenize(tweets[0], pattern1)



# Write a pattern that matches both mentions and hashtags

pattern2 = r"([#|@]\w+)"



# Use the pattern on the last tweet in the tweets list

regexp_tokenize(tweets[-1], pattern2)



# Use the TweetTokenizer to tokenize all tweets into one list

tknzr = TweetTokenizer()

all_tokens = [tknzr.tokenize(t) for t in tweets]

print(all_tokens)
